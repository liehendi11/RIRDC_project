{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 for twitter and 2 for patents2\n",
      "Loading packages and file paths declarations done...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initializes python packages and give paths to appropriate files to be used like categories file,\n",
    "the twitter file to be used alongwith the default stopword list (This is where our own created stoplist will be declared too).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import datetime, calendar, nltk, string, math, csv, glob, ntpath, os.path, re\n",
    "import numpy as np\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer; wnl = WordNetLemmatizer()\n",
    "import warnings, matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from rirdc_lib_old import cleanUp, calculate_seed\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\shilp\\Documents\\GitHub\\RIRDC_project\\files required'\n",
    "stopwords_file, categories_file = path+'\\\\stopwords.txt', path+'\\\\Category_DomainTerms.txt' # {(category, term)}\n",
    "out_fname= path+'\\\\new_data_file.csv'\n",
    "extra_cats_file=path+'\\\\extra_cats.txt'\n",
    "wordnet_path=path+'\\\\corpora\\\\wn-domains-3.2\\\\wn-domains-3.2-20070223'\n",
    "yago_cats_file=path+'\\\\yago_cats.txt'\n",
    "yago_path=path+'\\\\yagoWordnetDomains.tsv'\n",
    "\n",
    "n_topics, seed_ratio = 5, 0.25\n",
    "max_df, min_df = 0.5, 0.1 # For the VSM\n",
    "topic_groups=[]\n",
    "delimiter=','\n",
    "#from rirdc_lib_old import cleanUp, calculate_seed\n",
    "def keywithmaxval(d):\n",
    "    v=list(d.values());k=list(d.keys())\n",
    "    return k[v.index(max(v))]\n",
    "value=int(input(\"1 for twitter and 2 for patents\"))\n",
    "if value==1:\n",
    "    data_file=path+'\\\\results-20170725-145833.csv'\n",
    "elif value==2:\n",
    "    data_file=path+'\\\\Machine learning_out.txt'\n",
    "    delimiter='\\t'\n",
    "print('Loading packages and file paths declarations done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yago_cats={}\n",
    "category, words='',[]\n",
    "for line in open(yago_path, 'r'):\n",
    "    for item in line.split(\"\\t\"):\n",
    "        if item.startswith('<wordnet'):\n",
    "            if item.startswith('<wordnetDomain'):\n",
    "                categoryy = item.replace('<wordnetDomain_','')\n",
    "                category = categoryy.rstrip('>')\n",
    "            else:\n",
    "                words1 = item.replace('<wordnet_','')\n",
    "                words2 = ''.join([i for i in words1 if not i.isdigit()])\n",
    "                words3 = words2.rstrip('_>')\n",
    "                words=words3.replace('_',' ')\n",
    "    if category:\n",
    "        try:\n",
    "                yago_cats[category].append(words)\n",
    "        except KeyError:\n",
    "                yago_cats[category] = [words]\n",
    "out_yago_cats = open(yago_cats_file, 'w')\n",
    "for k in yago_cats.keys():\n",
    "    for v in yago_cats[k]:\n",
    "        out_yago_cats.write(k+\", \")\n",
    "        out_yago_cats.write(v+\"\\n\")\n",
    "out_yago_cats.close()\n",
    "print(\"Done creating YAGO categories..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Creating the cleaned version of tweet file...\n"
     ]
    }
   ],
   "source": [
    "#Creates a new file from the tweet file by getting rid of rows that are incomplete by appending their content to the \n",
    "#preceding row and deleting blank rows\n",
    "test_file=open(data_file,encoding=\"ISO-8859-1\")\n",
    "twitter_file = open(out_fname, 'w',encoding=\"ISO-8859-1\",newline='')\n",
    "reader = csv.reader(test_file, delimiter=delimiter)\n",
    "writer = csv.writer(twitter_file)\n",
    "next(reader, None)\n",
    "prev=reader\n",
    "for row in reader:\n",
    "    if row[0] in (None, \"\"):\n",
    "        continue\n",
    "    for term in row[3]:\n",
    "        if term.isalpha()==False or ord(term) > 127:\n",
    "            row[3]=row[3].replace(term,' ')\n",
    "        wnl.lemmatize(term,'n')\n",
    "    writer.writerow(row)\n",
    "test_file.close()\n",
    "twitter_file.close()\n",
    "print('Done Creating the cleaned version of tweet file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Loading the Wordnet domains.\\ns2d = []\\nfor i in open(wordnet_path, \\'r\\'):\\n    ssid, doms = i.strip().split(\\'\\t\\')\\n    doms = doms.split()\\n    for d in doms:\\n        s2d.append(d)\\nsynset2domains=list(set(s2d))\\nextra_cats={}\\n\\nfor syn in synset2domains:\\n    try:\\n        p_list,c_list,categories=[],[],[]\\n        obj=wn.synset(syn+\\'.n.01\\')\\n        topics=obj.topic_domains()\\n        parents=obj.hypernyms()\\n        children=obj.hyponyms()\\n        domains= [ topic.lemma_names() for topic in topics ] \\n        for sublist in domains:\\n            for item in sublist:\\n                    categories.append(str(item.replace(\\'_\\',\\' \\')))\\n        pwords = [ parent.lemma_names() for parent in parents ]\\n        for sublist in pwords:\\n            for item in sublist:\\n                    p_list.append(str(item.replace(\\'_\\',\\' \\')))\\n        cwords = [ child.lemma_names() for child in children]\\n        for sublist in cwords:\\n            for item in sublist:\\n                    c_list.append(str(item.replace(\\'_\\',\\' \\')))\\n        words=p_list+c_list\\n        if categories:\\n            for category in categories:\\n                extra_cats[category]=words\\n    except:\\n        pass\\n\\nout_extra_cats = open(extra_cats_file, \\'w\\')\\nfor k in extra_cats.keys():\\n    for v in extra_cats[k]:\\n        out_extra_cats.write(k+\", \")\\n        out_extra_cats.write(v+\"\\n\")\\nout_extra_cats.close()\\nprint(\"Done creating extra categories..\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Loading the Wordnet domains.\n",
    "s2d = []\n",
    "for i in open(wordnet_path, 'r'):\n",
    "    ssid, doms = i.strip().split('\\t')\n",
    "    doms = doms.split()\n",
    "    for d in doms:\n",
    "        s2d.append(d)\n",
    "synset2domains=list(set(s2d))\n",
    "extra_cats={}\n",
    "\n",
    "for syn in synset2domains:\n",
    "    try:\n",
    "        p_list,c_list,categories=[],[],[]\n",
    "        obj=wn.synset(syn+'.n.01')\n",
    "        topics=obj.topic_domains()\n",
    "        parents=obj.hypernyms()\n",
    "        children=obj.hyponyms()\n",
    "        domains= [ topic.lemma_names() for topic in topics ] \n",
    "        for sublist in domains:\n",
    "            for item in sublist:\n",
    "                    categories.append(str(item.replace('_',' ')))\n",
    "        pwords = [ parent.lemma_names() for parent in parents ]\n",
    "        for sublist in pwords:\n",
    "            for item in sublist:\n",
    "                    p_list.append(str(item.replace('_',' ')))\n",
    "        cwords = [ child.lemma_names() for child in children]\n",
    "        for sublist in cwords:\n",
    "            for item in sublist:\n",
    "                    c_list.append(str(item.replace('_',' ')))\n",
    "        words=p_list+c_list\n",
    "        if categories:\n",
    "            for category in categories:\n",
    "                extra_cats[category]=words\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "out_extra_cats = open(extra_cats_file, 'w')\n",
    "for k in extra_cats.keys():\n",
    "    for v in extra_cats[k]:\n",
    "        out_extra_cats.write(k+\", \")\n",
    "        out_extra_cats.write(v+\"\\n\")\n",
    "out_extra_cats.close()\n",
    "print(\"Done creating extra categories..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done moving tweets to their respective months and location files...\n"
     ]
    }
   ],
   "source": [
    "#Creates the equal number of files for the unique months and locations present \n",
    "#in the data file and dumps corresponding tweets to the appropriate ones.\n",
    "\n",
    "input_file = open(out_fname,encoding=\"ISO-8859-1\")\n",
    "date_list,country_list=[],[]\n",
    "reader = csv.reader(input_file, delimiter=',')\n",
    "\n",
    "#Creates a unique list of months and countries from tweets\n",
    "for row in reader:\n",
    "    created_at,*rest=row[1].split(' ')           #splits the 'Created at' column to retrieve 'month'\n",
    "    year,months,date=created_at.split('-')\n",
    "    month=calendar.month_name[int(months)]\n",
    "    if month not in date_list:\n",
    "       date_list.append(str(month))\n",
    "    if row[6] not in country_list:\n",
    "       country_list.append(str(row[6]))\n",
    "\n",
    "#Creates csv files with names after months\n",
    "for month in date_list:\n",
    "   outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'w', newline='',encoding=\"ISO-8859-1\")\n",
    "   writer = csv.writer(outfile, delimiter = ',')\n",
    "   writer.writerow([\"tweet_id\",\"month\",\"text\",\"country\"])\n",
    "   outfile.close()\n",
    " \n",
    "#Creates csv files with names after countries\n",
    "for country in country_list:\n",
    "   outfile = open(path+'\\Data{0:2s}'.format(str(country))+'.csv', 'w', newline='',encoding=\"ISO-8859-1\")\n",
    "   writer = csv.writer(outfile, delimiter = ',')\n",
    "   writer.writerow([\"tweet_id\",\"month\",\"text\",\"country\"])\n",
    "   outfile.close()\n",
    "    \n",
    "#Shifts rows of tweets to their respective csv files  -->  For Months\n",
    "for month1 in date_list:\n",
    "  with open(out_fname, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month1))+'.csv','a', newline='',encoding=\"ISO-8859-1\")\n",
    "    for row in reader:\n",
    "        if row[0] in (None, \"\"):\n",
    "          continue\n",
    "        created_at,*rest=row[1].split(' ')  \n",
    "        year,months,date=created_at.split('-')\n",
    "        month2=calendar.month_name[int(months)]\n",
    "        if month1==month2:\n",
    "            writer = csv.writer(outfile, delimiter =',')\n",
    "            writer.writerow([row[0],row[1],row[3],row[6]])\n",
    "    outfile.close()\n",
    " \n",
    "#Shifts rows of tweets to their respective csv files  -->  For Countries\n",
    "for country1 in country_list:\n",
    "  with open(out_fname, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(country1))+'.csv','a', newline='',encoding=\"ISO-8859-1\")\n",
    "    for row in reader:\n",
    "        if row[0] in (None, \"\"):\n",
    "          continue\n",
    "        country2=str(row[6])\n",
    "        if country1==country2:\n",
    "            writer = csv.writer(outfile, delimiter =',')\n",
    "            writer.writerow([row[0],row[1],row[3],row[6]])\n",
    "    outfile.close()\n",
    "print('Done moving tweets to their respective months and location files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the months found\n",
      "The top five bigrams are ['data', 'based', 'user', 'system', 'device']\n",
      "Clean-up for months done..., "
     ]
    }
   ],
   "source": [
    "#Tokenizes and remove stopwords from the tweet column of the data file for bigrams and unigrams --> For Months\n",
    "\n",
    "df=open(stopwords_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "stops=df.readlines(); df.close()\n",
    "stops = set([word.strip() for word in stops])                  #stores stopwords tokens from the stoplist in a list\n",
    "stops1=list(stops)\n",
    "stops2=''.join(stops1)\n",
    "tweet_id_m, text_m={},{}\n",
    "tweet_id_c,text_c={},{}\n",
    "docs,r=[],[]\n",
    "sentences=[]\n",
    "DocZ_m, DocZ_c, DocZ_m_wob, DocZ_c_wob = {},{},{},{} # will be used to find categories\n",
    "for month in date_list:\n",
    "    u,p,d,d_wob=[],[],[],[]\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'r', newline='',encoding=\"ISO-8859-1\")\n",
    "    reader = csv.reader(outfile, delimiter=',')\n",
    "    next(reader,None)\n",
    "    docs=list(reader);\n",
    "    for doc in docs:    \n",
    "        tmp = [t.strip() for t in doc]\n",
    "        try:\n",
    "            if len(tmp[2])>0: # Making sure not blanks\n",
    "                sentence = [word\n",
    "                    for word in nltk.word_tokenize(tmp[2].lower())\n",
    "                    if word not in string.punctuation\n",
    "                           and len(word)>3\n",
    "                           ]\n",
    "                filtered_word_list = sentence[:] #make a copy of the word_list\n",
    "                for term in sentence: # iterate over word_list\n",
    "                    if term in stops2: \n",
    "                        filtered_word_list.remove(term)\n",
    "                d_wob.append(tmp[2].lower())                  #Unigrams List\n",
    "                sentences.append(filtered_word_list)                  #Bigrams List\n",
    "                filtered=' '.join(filtered_word_list)\n",
    "                u.append(tmp[0]); p.append(tmp[2]);d.append(filtered)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_id_m[month]=u\n",
    "    text_m[month]=p \n",
    "    DocZ_m[month]=d\n",
    "    DocZ_m_wob[month]=d_wob\n",
    "   \n",
    "print(\"For the months found\")\n",
    "#print(\"The bigrams are -->\", DocZ_m)\n",
    "#print(\"The unigrams are -->\", DocZ_m_wob)\n",
    "\n",
    "#Gets the top five bigrams\n",
    "flat_list = [item for sublist in sentences for item in sublist]\n",
    "word_counter = {}\n",
    "for word in flat_list:\n",
    "     if word in word_counter:\n",
    "        word_counter[word] += 1\n",
    "     else:\n",
    "         word_counter[word] = 1\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "top_5 = popular_words[:5]\n",
    "print(\"The top five bigrams are\",top_5)\n",
    "del u,p,d,d_wob,sentence\n",
    "    \n",
    "del docs\n",
    "print(\"Clean-up for months done...,\", end = ' ', flush = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait (it's going to take a while): Loading Data, pre-Categories mapping ... \n"
     ]
    }
   ],
   "source": [
    "# Loading categories into category dictionary of set of terms for bigrams and unigrams for months\n",
    "\n",
    "print(\"Please wait (it's going to take a while): Loading Data for monrhs,\", end = ' ', flush = True)\n",
    "df=open(categories_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); categories = {}\n",
    "for cat in cats:\n",
    "    key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "    try:        \n",
    "          categories[key].add(term)\n",
    "    except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file\n",
    "del cats,df\n",
    "\n",
    "df=open(extra_cats_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); \n",
    "for cat in cats:\n",
    "    key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "    try:        \n",
    "          categories[key].add(term)\n",
    "    except:\n",
    "          categories[key] = set([term])  \n",
    "del cats,df\n",
    "\n",
    "df=open(yago_cats_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); \n",
    "for cat in cats:\n",
    "    key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "    try:        \n",
    "          categories[key].add(term)\n",
    "    except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file\n",
    "del cats,df\n",
    "\n",
    "print(\"pre-Categories mapping ... \", flush = True)    \n",
    "doc_cats_m,doc_cat_m = {}, {};\n",
    "doc_cats_m_wob,doc_cat_m_wob={},{};\n",
    "for month in date_list:\n",
    "    doc_cats_m[month],doc_cat_m[month]={},{} \n",
    "    for idx, doc in enumerate(DocZ_m[month]):\n",
    "        notInCategories,total=[],0\n",
    "        doc_cats_m[month][idx]={}\n",
    "        for cat, terms in categories.items():\n",
    "            for term in terms:\n",
    "                n = sum(1 for x in re.finditer(r\"\\b\"+term+r\"\\b\", doc))              #calculates the frequency of occurance of terms corresponding to terms for the category\n",
    "                total+=n\n",
    "                try:\n",
    "                    doc_cats_m[month][idx][cat] += n\n",
    "                except:\n",
    "                    doc_cats_m[month][idx][cat]= n\n",
    "        for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "                doc_cat_m[month][idx] = None\n",
    "                doc_cats_m[month][idx][cat] = 0.0\n",
    "                if month not in notInCategories:\n",
    "                    notInCategories.append(month)\n",
    "            else:\n",
    "                doc_cat_m[month][idx] = keywithmaxval(doc_cats_m[month][idx])           # category of doc[idx]\n",
    "                doc_cats_m[month][idx][cat] = doc_cats_m[month][idx][cat]/total          # Normalizing to category's ratio\n",
    "                \n",
    "del doc\n",
    "#print(doc_cat_m)\n",
    "print('%d/%d Documents were not in any of the categories for bigrams' %(len(notInCategories), len(DocZ_m)), flush = True)\n",
    "\n",
    "#For unigrams\n",
    "for month in date_list:\n",
    "    doc_cats_m_wob[month],doc_cat_m_wob[month]={},{} \n",
    "    for idx, doc in enumerate(DocZ_m_wob[month]):\n",
    "        notInCategories,total=[],0\n",
    "        doc_cats_m_wob[month][idx]={}\n",
    "        for cat, terms in categories.items():\n",
    "            for term in terms:\n",
    "                n = doc.count(term)\n",
    "                total+=n\n",
    "                try:\n",
    "                    doc_cats_m_wob[month][idx][cat] += n\n",
    "                except:\n",
    "                    doc_cats_m_wob[month][idx][cat]= n\n",
    "        for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "                doc_cat_m_wob[month][idx] = None\n",
    "                doc_cats_m_wob[month][idx][cat] = 0.0\n",
    "                if month not in notInCategories:\n",
    "                     notInCategories.append(month)\n",
    "            else:\n",
    "                doc_cat_m_wob[month][idx] = keywithmaxval(doc_cats_m_wob[month][idx])# category of doc[month][idx]\n",
    "                doc_cats_m_wob[month][idx][cat] = doc_cats_m_wob[month][idx][cat]/total # Normalizing to category's ratio\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------------------------>>>>>\")\n",
    "#print(doc_cat_m_wob)\n",
    "print('%d/%d Documents were not in any of the categories for unigrams' %(len(notInCategories), len(DocZ_m_wob)), flush = True)\n",
    "print('Done mapping categories...', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading categories into category dictionary of set of terms for bigrams and unigrams for countries\n",
    "\n",
    "print(\"Please wait (it's going to take a while): Loading Data for countries,\", end = ' ', flush = True)\n",
    "doc_cats_c,doc_cat_c = {}, {};\n",
    "doc_cats_c_wob,doc_cat_c_wob={},{};\n",
    "for country in country_list:\n",
    "    doc_cats_c[country],doc_cat_c[country]={},{} \n",
    "    for idx, doc in enumerate(DocZ_c[country]):\n",
    "        notInCategories,total=[],0\n",
    "        doc_cats_c[country][idx]={}\n",
    "        for cat, terms in categories.items():\n",
    "            for term in terms:\n",
    "                n = sum(1 for x in re.finditer(r\"\\b\"+term+r\"\\b\", doc)) \n",
    "                total+=n\n",
    "                try:\n",
    "                    doc_cats_c[country][idx][cat] += n\n",
    "                except:\n",
    "                    doc_cats_c[country][idx][cat]= n\n",
    "        for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "                doc_cat_c[country][idx] = None\n",
    "                doc_cats_c[country][idx][cat] = 0.0\n",
    "                if country not in notInCategories:\n",
    "                    notInCategories.append(country)\n",
    "            else:\n",
    "                doc_cat_c[country][idx] = keywithmaxval(doc_cats_c[country][idx])# category of doc[idx]\n",
    "                doc_cats_c[country][idx][cat] = doc_cats_c[country][idx][cat]/total # Normalizing to category's ratio\n",
    "                \n",
    "#print(doc_cat_c)\n",
    "print('%d/%d Documents were not in any of the categories for bigrams.' %(len(notInCategories), len(DocZ_c)), flush = True)\n",
    "\n",
    "for country in country_list:\n",
    "    doc_cats_c_wob[country],doc_cat_c_wob[country]={},{} \n",
    "    for idx, doc in enumerate(DocZ_c_wob[country]):\n",
    "        notInCategories,total=[],0\n",
    "        doc_cats_c_wob[country][idx]={}\n",
    "        for cat, terms in categories.items():\n",
    "            for term in terms:\n",
    "                n = doc.count(term)\n",
    "                total+=n\n",
    "                try:\n",
    "                    doc_cats_c_wob[country][idx][cat] += n\n",
    "                except:\n",
    "                    doc_cats_c_wob[country][idx][cat]= n\n",
    "        for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "                doc_cat_c_wob[country][idx] = None\n",
    "                doc_cats_c_wob[country][idx][cat] = 0.0\n",
    "                if month not in notInCategories:\n",
    "                    notInCategories.append(country)\n",
    "            else:\n",
    "                doc_cat_c_wob[country][idx] = keywithmaxval(doc_cats_c_wob[country][idx])# category of doc[idx]\n",
    "                doc_cats_c_wob[country][idx][cat] = doc_cats_c_wob[country][idx][cat]/total # Normalizing to category's ratio\n",
    "\n",
    "#print(\"------------------------------------------------------------------------------------------------------------------------------------------>>>>>\")\n",
    "#print(doc_cat_c_wob)\n",
    "print('%d/%d Documents were not in any of the categories for unigrams' %(len(notInCategories), len(DocZ_c_wob)), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates tf-idf vocab list\n",
    "print('Please wait calculating VSM ...', end = ' ', flush = True)\n",
    "dtm_tf_m,dtm_tf_c={},{}\n",
    "tf_terms_m,tf_terms_c={},{}\n",
    "tf_vectorizer_m, tf_vectorizer_c={},{}\n",
    "\n",
    "#For months\n",
    "for m in date_list:\n",
    "    tf_vectorizer_m[m] = CountVectorizer(strip_accents = 'unicode',stop_words = 'english',lowercase = True,token_pattern = r'\\b[a-zA-Z]{3,}\\b',max_df = 0.5,min_df=10)\n",
    "    try:\n",
    "        dtm_tf_m[m]=tf_vectorizer_m[m].fit_transform(DocZ_m[m])   #a sparse matrix\n",
    "        #print(\"stm_tf is\");print(dtm_tf[c])\n",
    "        tf_terms_m[m] = tf_vectorizer_m[m].get_feature_names()      #a list\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#For countries\n",
    "for c in country_list:\n",
    "    tf_vectorizer_c[c] = CountVectorizer(strip_accents = 'unicode',stop_words = 'english',lowercase = True,token_pattern = r'\\b[a-zA-Z]{3,}\\b',max_df = 0.5, min_df=10)\n",
    "    try:\n",
    "        dtm_tf_c[c]=tf_vectorizer_c[c].fit_transform(DocZ_c[c])\n",
    "        #print(\"stm_tf is\");print(dtm_tf[c])\n",
    "        tf_terms_c[c] = tf_vectorizer_c[c].get_feature_names()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Please wait calculating LDA topics ...', end = ' ', flush = True)\n",
    "with warnings.catch_warnings():\n",
    "    lda_tf_m,vsm_topics_m,doc_topic_m={},{},{}\n",
    "    lda_tf_c,vsm_topics_c,doc_topic_c={},{},{}\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    for m in date_list:\n",
    "        lda= LatentDirichletAllocation(n_topics=10, max_iter=11,learning_method='batch', learning_offset=50., random_state=0)\n",
    "        try:\n",
    "            lda_tf_m[m]=lda.fit(dtm_tf_m[m])\n",
    "            vsm_topics_m[m]=lda_tf_m[m].transform(dtm_tf_m[m])\n",
    "            doc_topic_m[m]=[a.argmax()+1 for a in vsm_topics_m[m]] # topic of docs\n",
    "        except:\n",
    "            pass\n",
    "    print(doc_topic_m)\n",
    "    for c in country_list:\n",
    "        lda= LatentDirichletAllocation(n_topics=10, max_iter=11,learning_method='batch', learning_offset=50., random_state=0)\n",
    "        try:\n",
    "            lda_tf_c[c]=lda.fit(dtm_tf_c[c])\n",
    "            vsm_topics_c[c]=lda_tf_c[c].transform(dtm_tf_c[c])\n",
    "            doc_topic_c[c]=[a.argmax()+1 for a in vsm_topics_c[c]]\n",
    "        except:\n",
    "            pass\n",
    "print('Done Assigning Topics', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Please wait visualising LDA topics for different months ...', end = ' ', flush = True)\n",
    "for m in date_list:\n",
    "    print(\"For month -> \",m)\n",
    "    display(pyLDAvis.sklearn.prepare(lda_tf_m[m], dtm_tf_m[m], tf_vectorizer_m[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Please wait calculating LDA topics for different countries...', end = ' ', flush = True)\n",
    "for c in country_list:\n",
    "    print(\"For country -> \",c)\n",
    "    display(pyLDAvis.sklearn.prepare(lda_tf_c[c], dtm_tf_c[c], tf_vectorizer_c[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clusters together nearest topics\n",
    "print('Please wait clustering topics ...', flush = True)\n",
    "dict_topics_groups_m,dict_topics_groups_c={},{}\n",
    "for m in date_list:\n",
    "    data=pyLDAvis.sklearn.prepare(lda_tf_m[m], dtm_tf_m[m], tf_vectorizer_m[m])\n",
    "    df = pd.DataFrame(data.topic_info).sort_values('Freq', ascending=False) \n",
    "    var=df.head(100)[['Freq', 'Term','Category']].drop_duplicates(subset='Category')         #gets top frequent terms for each unique Category i.e. Topic\n",
    "    mydict = dict(zip(var.Category,var.Term))\n",
    "    del mydict['Default']\n",
    "    for k in list(mydict):\n",
    "        new_k=k.replace(\"Topic\",\"\")            \n",
    "        mydict[new_k] = mydict[k]     \n",
    "        del mydict[k]\n",
    "    #print(data.topic_info)\n",
    "    topic_xdist, topic_ydist=data.topic_coordinates.x, data.topic_coordinates.y\n",
    "    dist_matrix=np.column_stack((topic_xdist,topic_ydist))\n",
    "    kmeans = KMeans(n_clusters=5)      #specify number of clusters here\n",
    "    kmeans.fit(dist_matrix)\n",
    "    labels = kmeans.labels_\n",
    "    topics_groups_old=[]     #with index starting from 0\n",
    "    new_labels=[]\n",
    "    for label in labels:\n",
    "        new_labels.append(np.asscalar(label))\n",
    "    for k in range(0,len(new_labels)):\n",
    "        iter=0\n",
    "        for i in range(0,k):\n",
    "            if new_labels[i]==new_labels[k]:\n",
    "                break\n",
    "            iter=iter+1\n",
    "        if iter!=k:\n",
    "            for temp in topics_groups_old:\n",
    "                for t in temp:\n",
    "                    if t==iter:\n",
    "                        s=topics_groups_old.index(temp)\n",
    "                        topics_groups_old[s].append(k)\n",
    "            continue\n",
    "        temp_list=[]\n",
    "        temp_list.append(k)\n",
    "        topics_groups_old.append(temp_list)\n",
    "\n",
    "    topics_groups=[]\n",
    "    for temp in topics_groups_old:\n",
    "                temp_list=[]\n",
    "                for t in temp:\n",
    "                    temp_list.append(t+1)\n",
    "                topics_groups.append(temp_list)\n",
    "    dict_topics_groups_m[m]=topics_groups\n",
    "#print(dict_topics_groups_m)\n",
    "\n",
    "print(\"Done Clustering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clustering for countries\n",
    "for c in country_list:\n",
    "    data_c=pyLDAvis.sklearn.prepare(lda_tf_c[c], dtm_tf_c[c], tf_vectorizer_c[c])\n",
    "    df = pd.DataFrame(data.topic_info).sort('Freq', ascending=False)\n",
    "    var=df.head(100)[['Freq', 'Term','Category']].drop_duplicates(subset='Category')\n",
    "    mydict_c = dict(zip(var.Category,var.Term))\n",
    "    del mydict_c['Default']\n",
    "    for k in list(mydict_c):\n",
    "        new_k=k.replace(\"Topic\",\"\")\n",
    "        mydict_c[new_k] = mydict_c[k]\n",
    "        del mydict_c[k]\n",
    "    print(mydict_c)\n",
    "    topic_xdist_c, topic_ydist_c=data_c.topic_coordinates.x, data_c.topic_coordinates.y\n",
    "    dist_matrix_c=np.column_stack((topic_xdist_c,topic_ydist_c))\n",
    "    kmeans_c = KMeans(n_clusters=5)      #specify number of clusters here\n",
    "    kmeans_c.fit(dist_matrix_c)\n",
    "    labels_c = kmeans_c.labels_\n",
    "    topics_groups_old=[]     #with index starting from 0\n",
    "    new_labels=[]\n",
    "    for label in labels_c:\n",
    "        new_labels.append(np.asscalar(label))\n",
    "    for k in range(0,len(new_labels)):\n",
    "        iter=0\n",
    "        for i in range(0,k):\n",
    "            if new_labels[i]==new_labels[k]:\n",
    "                break\n",
    "            iter=iter+1\n",
    "        if iter!=k:\n",
    "            for temp in topics_groups_old:\n",
    "                for t in temp:\n",
    "                    if t==iter:\n",
    "                        s=topics_groups_old.index(temp)\n",
    "                        topics_groups_old[s].append(k)\n",
    "            continue\n",
    "        temp_list=[]\n",
    "        temp_list.append(k)\n",
    "        topics_groups_old.append(temp_list)\n",
    "\n",
    "    topics_groups_c=[]\n",
    "    for temp in topics_groups_old:\n",
    "                temp_list=[]\n",
    "                for t in temp:\n",
    "                    temp_list.append(t+1)\n",
    "                topics_groups_c.append(temp_list)\n",
    "    dict_topics_groups_c[c]=topics_groups_c\n",
    "print(dict_topics_groups_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculates a score for each domain for each document in each month\n",
    "\n",
    "topic_categories_freq, topic_categories_score = {}, {}\n",
    "for month in date_list:\n",
    "    topic_categories_freq[month], topic_categories_score[month] = {}, {}\n",
    "    for topic in dict_topics_groups_m[month]:\n",
    "        temp=[]\n",
    "        topic_categories_freq[month][str(topic)], topic_categories_score[month][str(topic)] = {}, {}\n",
    "        doc_idx = [idx for idx, tpc in enumerate(doc_topic_m[month]) if tpc in topic] # all document index in the chosen topic   \n",
    "        for idx in doc_idx:\n",
    "            C = doc_cat_m[month][idx]\n",
    "            temp.append(C)\n",
    "            if C: #not None\n",
    "                try:\n",
    "                    topic_categories_freq[month][str(topic)][C]+=1\n",
    "                    topic_categories_score[month][str(topic)][C]+=doc_cats_m[month][idx][C]\n",
    "                except:\n",
    "                    topic_categories_freq[month][str(topic)][C]=1\n",
    "                    topic_categories_score[month][str(topic)][C]=doc_cats_m[month][idx][C]\n",
    "            else:\n",
    "                try:\n",
    "                    topic_categories_freq[month][str(topic)]['None']+=1\n",
    "                except:\n",
    "                    topic_categories_freq[month][str(topic)]['None']=1  \n",
    "        for k,v in categories.items():\n",
    "            if k not in temp:\n",
    "                topic_categories_freq[month][str(topic)][k]=0.0\n",
    "                topic_categories_score[month][str(topic)][k]=0.0\n",
    "    # Normalizing scores\n",
    "        for C in topic_categories_freq[month][str(topic)].keys():\n",
    "            if C is not 'None':\n",
    "                N = topic_categories_freq[month][str(topic)][C]\n",
    "                if N !=0:\n",
    "                    topic_categories_score[month][str(topic)][C] = topic_categories_score[month][str(topic)][C]/N\n",
    "                else:\n",
    "                    topic_categories_score[month][str(topic)][C] = 0.0\n",
    "        if topic_categories_freq[month][str(topic)]['agriculture'] == topic_categories_freq[month][str(topic)]['finance'] == topic_categories_freq[month][str(topic)]['mining'] == 0.0:\n",
    "            topic_categories_freq[month][str(topic)]['None']=1 \n",
    "        if topic_categories_score[month][str(topic)]['agriculture'] == topic_categories_score[month][str(topic)]['finance'] == topic_categories_score[month][str(topic)]['mining'] == 0.0:\n",
    "            topic_categories_score[month][str(topic)]['None']=1 \n",
    "#print(topic_categories_freq)\n",
    "#print(topic_categories_score)\n",
    "print ('Done assigning scores...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculates a score for each domain in each country\n",
    "topic_categories_freq_c, topic_categories_score_c = {}, {}\n",
    "qwerty=[]\n",
    "for country in country_list:\n",
    "    topic_categories_freq_c[country], topic_categories_score_c[country] = {}, {}\n",
    "    for topic in dict_topics_groups_c[country]:\n",
    "        temp=[]\n",
    "        topic_categories_freq_c[country][str(topic)], topic_categories_score_c[country][str(topic)] = {}, {}\n",
    "        doc_idx = [idx for idx, tpc in enumerate(doc_topic_c[country]) if tpc in topic] # all document index in the chosen topic    \n",
    "        for idx in doc_idx:\n",
    "        C = doc_cat_c[country][idx]\n",
    "        temp.append(C)\n",
    "        if C: #not None\n",
    "            try:\n",
    "                topic_categories_freq_c[country][str(topic)][C]+=1\n",
    "                topic_categories_score_c[country][str(topic)][C]+=doc_cats_c[country][idx][C]\n",
    "            except:\n",
    "                topic_categories_freq_c[country][str(topic)][C]=1\n",
    "                topic_categories_score_c[country][str(topic)][C]=doc_cats_c[country][idx][C]\n",
    "        else:\n",
    "            try:\n",
    "                topic_categories_freq_c[country][str(topic)]['None']+=1\n",
    "            except:\n",
    "                topic_categories_freq_c[country][str(topic)]['None']=1  \n",
    "      for k,v in categories.items():\n",
    "        if k not in temp:\n",
    "            topic_categories_freq_c[country][str(topic)][k]=0.0\n",
    "    # Normalizing scores\n",
    "      for C in topic_categories_freq_c[country][str(topic)].keys():\n",
    "        if C is not 'None':\n",
    "            N = topic_categories_freq_c[country][str(topic)][C]\n",
    "            if N !=0:\n",
    "                topic_categories_score_c[country][str(topic)][C] = topic_categories_score_c[country][str(topic)][C]/N\n",
    "            else:\n",
    "                topic_categories_score_c[country][str(topic)][C] = 0.0\n",
    "      if topic_categories_freq_c[country][str(topic)]['agriculture'] == topic_categories_freq_c[country][str(topic)]['finance'] == topic_categories_freq_c[country][str(topic)]['mining'] == 0.0:\n",
    "        topic_categories_freq_c[country][str(topic)]['None']=1 \n",
    "#print(topic_categories_freq_c)\n",
    "#print(topic_categories_score_c)\n",
    "print ('Done calculating score..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates a pie chart and saves it as a png file to be loaded later\n",
    "fileNameTemplate = path+'\\Plot{0:2s}'\n",
    "for month in date_list:\n",
    "    try:\n",
    "        print(\"For month\",month)\n",
    "        for itr,topic in enumerate(dict_topics_groups_m[month]): # e.g. 4 = [4,3,7,11]\n",
    "            categoryy_list=list(topic_categories_freq[month][topic].keys())\n",
    "            print('For Topic = ',topic)\n",
    "            print('Visualization based on Category frequency: ', flush = True)\n",
    "            y=np.array(list(topic_categories_freq[month][str(topic)].values()))\n",
    "            percent = 100.*y/y.sum()\n",
    "            indices=np.where(percent == 0)[0].tolist()\n",
    "            porcent=np.delete(percent,indices)\n",
    "            np.asarray(categoryy_list)\n",
    "            category_list=np.delete(categoryy_list,indices).tolist()\n",
    "            labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(category_list, porcent)]\n",
    "            #labels = list(topic_categories_freq_c[country][str(topic)].keys())\n",
    "            sizes = list(topic_categories_freq[month][str(topic)].values())\n",
    "            cs=cm.Set1(np.arange(len(labels)))\n",
    "            explode = (0.1, 0, 0, 0)  # explode 1st slice: Change this anyway you like\n",
    "            patches,text=plt.pie(sizes, colors=cs, startangle=140)\n",
    "            plt.axis('equal')\n",
    "            sort_legend = True\n",
    "            if sort_legend:\n",
    "                patches, labels, dummy =  zip(*sorted(zip(patches, labels, sizes),\n",
    "                                                  key=lambda x: x[2],\n",
    "                                                  reverse=True))\n",
    "\n",
    "            plt.legend(patches, labels, loc='left center', bbox_to_anchor=(-0.1, 1.),\n",
    "                   fontsize=8)\n",
    "\n",
    "            plt.savefig(fileNameTemplate.format(str(topic)+'-'+month+'.png'), format='png', bbox_inches='tight')\n",
    "            plt.show()\n",
    "    except:\n",
    "        pass\n",
    "print('Done Visualising and saving pie charts...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates a pie chart and saves it as a png file to be loaded later --> for countries\n",
    "fileNameTemplate = path+'\\Plot{0:2s}'\n",
    "for country in country_list:\n",
    "    try:\n",
    "        print(\"For country\",country)\n",
    "        for itr,topic in enumerate(dict_topics_groups_c[country]): # e.g. 4 = [4,3,7,11]\n",
    "            print('For Topic = ',topic)\n",
    "            print('Visualization based on Category frequency: ', flush = True)\n",
    "            categoryy_list=list(topic_categories_freq[month][topic].keys())\n",
    "            y=np.array(list(topic_categories_freq_c[country][str(topic)].values()))\n",
    "            percent = 100.*y/y.sum()\n",
    "            indices=np.where(percent == 0)[0].tolist()\n",
    "            porcent=np.delete(percent,indices)\n",
    "            np.asarray(categoryy_list)\n",
    "            category_list=np.delete(categoryy_list,indices).tolist()\n",
    "            labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(category_list, porcent)]\n",
    "            print(list(topic_categories_freq_c[country][str(topic)].values()))\n",
    "            sizes = list(topic_categories_freq_c[country][str(topic)].values())\n",
    "            cs=cm.Set1(np.arange(len(labels)))\n",
    "            explode = (0.1, 0, 0, 0)  # explode 1st slice: Change this anyway you like\n",
    "            patches,text=plt.pie(sizes, colors=cs, startangle=140)\n",
    "            plt.axis('equal')\n",
    "            sort_legend = True\n",
    "            if sort_legend:\n",
    "                patches, labels, dummy =  zip(*sorted(zip(patches, labels, sizes),\n",
    "                                                  key=lambda x: x[2],\n",
    "                                                  reverse=True))\n",
    "\n",
    "            plt.legend(patches, labels, loc='left center', bbox_to_anchor=(-0.1, 1.),\n",
    "                   fontsize=8)\n",
    "\n",
    "            plt.savefig(fileNameTemplate.format(str(topic)+'-'+country+'.png'), format='png',bbox_inches='tight')\n",
    "            plt.show()\n",
    "    except:\n",
    "        pass\n",
    "print('Done Visualising and saving pie charts...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loads back the saved png pie charts to be displayed over month-topic groups axes\n",
    "\n",
    "n_clusters=5      #the one specified in kmm\n",
    "fig, axarr = plt.subplots(nrows=len(date_list),ncols= n_clusters, sharex='col', sharey='row',  figsize=(40,5))\n",
    "#When there's a single month detected, use 1D array for co-ordinates\n",
    "r=0\n",
    "c=0\n",
    "if len(date_list)==1:\n",
    "    axarr[0].yaxis.set_label_position(\"left\")\n",
    "    axarr[0].set_ylabel(date_list[0])\n",
    "    c=0\n",
    "    for col in dict_topics_groups_m[date_list[0]]:\n",
    "        ttl=''\n",
    "        for top in col:\n",
    "            ttl=ttl+mydict[str(top)]+','\n",
    "        axarr[c].set_title(ttl)\n",
    "        img=fileNameTemplate.format(str(col))+'-'+date_list[0]+'.png'\n",
    "        o=axarr[c].imshow(mpimg.imread(img),interpolation='nearest', aspect='auto')\n",
    "        plt.setp(axarr[c].get_xticklabels(), visible=False)\n",
    "        plt.setp(axarr[c].get_yticklabels(), visible=False)\n",
    "        c+=1\n",
    "    \n",
    "#else use numpy multi-dimesnional array\n",
    "else:\n",
    "    r=0\n",
    "    for row in date_list:\n",
    "        c=0\n",
    "        axarr[r,0].yaxis.set_label_position(\"left\")\n",
    "        axarr[r,0].set_ylabel(row)\n",
    "        for col in dict_topics_groups_m[row]:  \n",
    "            ttl=''\n",
    "            for top in col:\n",
    "                ttl=ttl+mydict[str(top)]+','\n",
    "            #axarr[0,c].yaxis.set_label_position(\"top\")\n",
    "            axarr[r,c].set_title(ttl)\n",
    "            img=fileNameTemplate.format(str(col))+'-'+row+'.png'\n",
    "            #if os.path.isfile(img):\n",
    "            #print(\"row\");print(r);print(\"column\"); print(c)\n",
    "            o=axarr[r, c].imshow(mpimg.imread(img),interpolation='nearest',aspect='auto')\n",
    "            plt.setp(axarr[r,c].get_xticklabels(), visible=False)\n",
    "            plt.setp(axarr[r,c].get_yticklabels(), visible=False)\n",
    "            c+=1\n",
    "        r+=1\n",
    "plt.show()\n",
    "print(\"Done plotting for months...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loads back the saved png pie charts to be displayed over month-topic groups axes\n",
    "\n",
    "n_clusters=5      #the one specified in kmm\n",
    "fig, axarr = plt.subplots(nrows=len(country_list),ncols= n_clusters, sharex='col', sharey='row',  figsize=(40,15))\n",
    "r=0\n",
    "c=0\n",
    "if len(country_list)==1:\n",
    "    axarr[0].yaxis.set_label_position(\"left\")\n",
    "    axarr[0].set_ylabel(country_list[0])\n",
    "    c=0\n",
    "    for col in dict_topics_groups_c[country_list[0]]: \n",
    "        ttl=''\n",
    "        for top in col:\n",
    "            ttl=ttl+mydict_c[str(top)]+','\n",
    "        axarr[c].set_title(ttl)\n",
    "        img=fileNameTemplate.format(str(col))+'-'+country_list[0]+'.png'\n",
    "        print(img)\n",
    "        o=axarr[c].imshow(mpimg.imread(img),interpolation='nearest', aspect='auto')\n",
    "        plt.setp(axarr[c].get_xticklabels(), visible=False)\n",
    "        plt.setp(axarr[c].get_yticklabels(), visible=False)\n",
    "        c+=1\n",
    "else:\n",
    "    r=0\n",
    "    for row in country_list:\n",
    "        c=0\n",
    "        axarr[r,0].yaxis.set_label_position(\"left\")\n",
    "        axarr[r,0].set_ylabel(row)\n",
    "        for col in dict_topics_groups_c[row]:  \n",
    "            ttl=''\n",
    "            for top in col:\n",
    "                ttl=ttl+mydict_c[str(top)]+','\n",
    "            #axarr[0,c].yaxis.set_label_position(\"top\")\n",
    "            axarr[r,c].set_title(ttl)\n",
    "            img=fileNameTemplate.format(str(col))+'-'+row+'.png'\n",
    "            #if os.path.isfile(img):\n",
    "            #print(\"row\");print(r);print(\"column\"); print(c)\n",
    "            o=axarr[r, c].imshow(mpimg.imread(img),interpolation='nearest', aspect='auto')\n",
    "            plt.setp(axarr[r,c].get_xticklabels(), visible=False)\n",
    "            plt.setp(axarr[r,c].get_yticklabels(), visible=False)\n",
    "            c+=1\n",
    "        r+=1\n",
    "plt.show()\n",
    "print(\"Done plotting for months...\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
