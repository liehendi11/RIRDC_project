{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shilp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 for twitter and 2 for patents2\n",
      "Loading packages and file paths declarations done...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initializes python packages and give paths to appropriate files to be used like categories file,\n",
    "the twitter file to be used alongwith the default stopword list (This is where our own created stoplist will be declared too).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import datetime, calendar, nltk, string, math, csv, glob, ntpath, os.path\n",
    "import numpy as np\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer; wnl = WordNetLemmatizer()\n",
    "import warnings, matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from rirdc_lib_old import cleanUp, calculate_seed\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\shilp\\Documents\\GitHub\\RIRDC_project\\files required' \n",
    "stopwords_file, categories_file = path+'\\\\stopwords.txt', path+'\\\\Category_DomainTerms.txt' # {(category, term)}\n",
    "out_fname = path+'\\\\new_data_file.csv'\n",
    "extra_cats_file=path+'\\\\extra_cats.txt'\n",
    "#Give the wordnet path here\n",
    "wordnet_path=path+'\\\\corpora\\\\wn-domains-3.2\\\\wn-domains-3.2-20070223'\n",
    "yago_cats_file=path+'\\\\yago_cats.txt'\n",
    "\n",
    "yago_path=path+'\\\\yagoWordnetDomains.tsv'\n",
    "n_topics, seed_ratio = 5, 0.25\n",
    "max_df, min_df = 0.5, 0.1 # For the VSM\n",
    "topic_groups=[]\n",
    "\n",
    "def keywithmaxval(d):\n",
    "    v=list(d.values());k=list(d.keys())\n",
    "    return k[v.index(max(v))]\n",
    "\n",
    "value=int(input(\"1 for twitter and 2 for patents\"))\n",
    "if value==1:\n",
    "    data_file=path+'\\\\results-20170725-145833.csv'\n",
    "elif value==2:\n",
    "    data_file=path+'\\\\Machine Learning_out.txt'\n",
    "print('Loading packages and file paths declarations done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fdc913922aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Creates a new file from the tweet file by getting rid of rows that are incomplete by appending their content to the \n",
    "#preceding row and deleting blank rows\n",
    "test_file=open(data_file,encoding=\"ISO-8859-1\")\n",
    "twitter_file = open(out_fname, 'w',encoding=\"ISO-8859-1\",newline='')\n",
    "reader = csv.reader(test_file, delimiter=',')\n",
    "writer = csv.writer(twitter_file)\n",
    "next(reader, None)\n",
    "for row in reader:\n",
    "    if row[0] in (None, \"\"):\n",
    "        continue\n",
    "    for term in row[3]:\n",
    "        if term.isalpha()==False:\n",
    "            try:\n",
    "                if math.isnan(float(term)):\n",
    "                    print(\"Entered\")\n",
    "                    row[3]=row[3].replace(term,' ')\n",
    "            except:\n",
    "                    pass\n",
    "        if ord(term) > 127:\n",
    "            row[3]=row[3].replace(term,' ')\n",
    "        wnl.lemmatize(term,'n')\n",
    "    writer.writerow(row)\n",
    "test_file.close()\n",
    "twitter_file.close()\n",
    "print('Done Creating the cleaned version of tweet file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the Wordnet domains.\n",
    "s2d = []\n",
    "for i in open(wordnet_path, 'r'):\n",
    "    ssid, doms = i.strip().split('\\t')\n",
    "    doms = doms.split()\n",
    "    for d in doms:\n",
    "        s2d.append(d)\n",
    "synset2domains=list(set(s2d))\n",
    "extra_cats={}\n",
    "\n",
    "for syn in synset2domains:\n",
    "    try:\n",
    "        p_list,c_list,categories=[],[],[]\n",
    "        obj=wn.synset(syn+'.n.01')\n",
    "        topics=obj.topic_domains()\n",
    "        parents=obj.hypernyms()\n",
    "        children=obj.hyponyms()\n",
    "        domains= [ topic.lemma_names() for topic in topics ] \n",
    "        for sublist in domains:\n",
    "            for item in sublist:\n",
    "                    categories.append(str(item.replace('_',' ')))\n",
    "        pwords = [ parent.lemma_names() for parent in parents ]\n",
    "        for sublist in pwords:\n",
    "            for item in sublist:\n",
    "                    p_list.append(str(item.replace('_',' ')))\n",
    "        cwords = [ child.lemma_names() for child in children]\n",
    "        for sublist in cwords:\n",
    "            for item in sublist:\n",
    "                    c_list.append(str(item.replace('_',' ')))\n",
    "        words=p_list+c_list\n",
    "        if categories:\n",
    "            for category in categories:\n",
    "                extra_cats[category]=words\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "out_extra_cats = open(extra_cats_file, 'w')\n",
    "for k in extra_cats.keys():\n",
    "    for v in extra_cats[k]:\n",
    "        out_extra_cats.write(k+\", \")\n",
    "        out_extra_cats.write(v+\"\\n\")\n",
    "out_extra_cats.close()\n",
    "print(\"Done creating extra categories..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tokenizes and remove stopwords from the tweet column of the data file for bigrams and unigrams --> For Months\n",
    "\n",
    "input_file = open(out_fname,encoding=\"ISO-8859-1\")\n",
    "date_list,country_list=[],[]\n",
    "reader = csv.reader(input_file, delimiter=',')\n",
    "\n",
    "#Creates a unique list of months and countries from tweets\n",
    "for row in reader:\n",
    "    created_at,*rest=row[1].split(' ')           #splits the 'Created at' column to retrieve 'month'\n",
    "    year,months,date=created_at.split('-')\n",
    "    month=calendar.month_name[int(months)]\n",
    "    if month not in date_list:\n",
    "       date_list.append(str(month))\n",
    "    \n",
    "#Creates csv files with names after months\n",
    "for month in date_list:\n",
    "   outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'w', newline='',encoding=\"ISO-8859-1\")\n",
    "   writer = csv.writer(outfile, delimiter = ',')\n",
    "   writer.writerow([\"tweet_id\",\"month\",\"text\",\"country\"])\n",
    "   outfile.close()\n",
    "    \n",
    "#Shifts rows of tweets to their respective csv files  -->  For Months\n",
    "for month1 in date_list:\n",
    "  with open(out_fname, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month1))+'.csv','a', newline='',encoding=\"ISO-8859-1\")\n",
    "    for row in reader:\n",
    "        if row[0] in (None, \"\"):\n",
    "          continue\n",
    "        created_at,*rest=row[1].split(' ')  \n",
    "        year,months,date=created_at.split('-')\n",
    "        month2=calendar.month_name[int(months)]\n",
    "        if month1==month2:\n",
    "            writer = csv.writer(outfile, delimiter =',')\n",
    "            writer.writerow(row)\n",
    "    outfile.close()\n",
    "print('Done moving tweets to their respective months and location files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tokenizes and remove stopwords from the tweet column of the data file for bigrams and unigrams --> For Months\n",
    "\n",
    "df=open(stopwords_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "stops=df.readlines(); df.close()\n",
    "stops = set([word.strip() for word in stops])                  #stores stopwords tokens from the stoplist in a list\n",
    "stops1=list(stops)\n",
    "stops2=''.join(stops1)\n",
    "tweet_id_m, text_m={},{}\n",
    "tweet_id_c,text_c={},{}\n",
    "docs,r=[],[]\n",
    "sentences=[]\n",
    "DocZ_m={} # will be used to find categories\n",
    "for month in date_list:\n",
    "    u,p,d=[],[],[]\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'r', newline='',encoding=\"ISO-8859-1\")\n",
    "    reader = csv.reader(outfile, delimiter=',')\n",
    "    next(reader,None)\n",
    "    docs=list(reader);\n",
    "    for doc in docs:    \n",
    "        tmp = [t.strip() for t in doc]\n",
    "        try:\n",
    "            if len(tmp[3])>0: # Making sure not blanks\n",
    "                sentence = [word\n",
    "                    for word in nltk.word_tokenize(tmp[3].lower())\n",
    "                    if word not in string.punctuation\n",
    "                           and len(word)>3\n",
    "                           ]\n",
    "                filtered_word_list = sentence[:] #make a copy of the word_list\n",
    "                for term in sentence: # iterate over word_list\n",
    "                    if term in stops2: \n",
    "                        filtered_word_list.remove(term)\n",
    "                sentences.append(filtered_word_list)                  #Bigrams List\n",
    "                filtered=' '.join(filtered_word_list)\n",
    "                u.append(tmp[0]); p.append(tmp[3]);d.append(filtered)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_id_m[month]=u\n",
    "    text_m[month]=p \n",
    "    DocZ_m[month]=d\n",
    "   \n",
    "print(\"For the months found\")\n",
    "#print(\"The bigrams are -->\", DocZ_m)\n",
    "\n",
    "del docs\n",
    "print(\"Clean-up for months done...,\", end = ' ', flush = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Please wait (it's going to take a while): Loading Data,\", end = ' ', flush = True)\n",
    "df=open(categories_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); categories = {}\n",
    "for cat in cats:\n",
    "     key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "     try:        \n",
    "          categories[key].add(term)\n",
    "     except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file\n",
    "            \n",
    "df=open(extra_cats_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); \n",
    "for cat in cats:\n",
    "     key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "     try:        \n",
    "          categories[key].add(term)\n",
    "     except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file  \n",
    "del cats,df\n",
    "df=open(yago_cats_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); \n",
    "for cat in cats:\n",
    "     key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "     try:        \n",
    "          categories[key].add(term)\n",
    "     except:\n",
    "          categories[key] = set([term])  \n",
    "print(\"pre-Categories mapping ... \", flush = True)    \n",
    "doc_cats_m,doc_cat_m = {}, {}; del cats, df\n",
    "doc_cats_m_wob,doc_cat_m_wob={},{};\n",
    "high_freq={}\n",
    "for month in date_list:\n",
    "   doc_cats_m[month],doc_cat_m[month],high_freq[month]={},{} ,{}\n",
    "   for idx, doc in enumerate(DocZ_m[month]):\n",
    "       notInCategories,total=[],0\n",
    "       doc_cats_m[month][idx]={}\n",
    "       for cat, terms in categories.items():\n",
    "          for term in terms:\n",
    "             n = doc.count(term)              #calculates the frequency of occurance of terms corresponding to terms for the category\n",
    "             total+=n\n",
    "             try:\n",
    "                doc_cats_m[month][idx][cat] += n\n",
    "             except:\n",
    "                doc_cats_m[month][idx][cat]= n\n",
    "       for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "               doc_cat_m[month][idx] = None\n",
    "               doc_cats_m[month][idx][cat] = 0.0\n",
    "               if month not in notInCategories:\n",
    "                  notInCategories.append(month)\n",
    "            else:\n",
    "             doc_cat_m[month][idx] = keywithmaxval(doc_cats_m[month][idx])           # category of doc[idx]\n",
    "             doc_cats_m[month][idx][cat] = doc_cats_m[month][idx][cat]/total          # Normalizing to category's ratio\n",
    "                \n",
    "del doc\n",
    "#print(doc_cat_m)\n",
    "print('%d/%d Documents were not in any of the categories.' %(len(notInCategories), len(DocZ_m)), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculates a score for each domain for each document in each month\n",
    "\n",
    "topic_categories_freq, topic_categories_score = {}, {}\n",
    "for month in date_list:\n",
    "#topics_groups = [[1,12],[2,18],[3,7,9,14,15,19],[4],[5],[6,8,11],[10,13,16,17],[20]] # Change this depending on the visualization result; PS: this is \"LDAtopic+1\"\n",
    "#topics_scores, seeds, seeds_score, clusters, centroids, mse = {}, {}, {}, {}, {}, {}\n",
    "      topic_categories_freq[month], topic_categories_score[month] = {}, {}\n",
    "      temp=[]  \n",
    "      for k,v in doc_cat_m[month].items():\n",
    "        C=doc_cat_m[month][k]\n",
    "        temp.append(doc_cat_m[month][k])\n",
    "        if C: #not None\n",
    "            try:\n",
    "                topic_categories_freq[month][C]+=1\n",
    "                topic_categories_score[month][C]+=doc_cats_m[month][k][C]\n",
    "            except:\n",
    "                topic_categories_freq[month][C]=1\n",
    "                topic_categories_score[month][C]=doc_cats_m[month][k][C]\n",
    "        else:\n",
    "            try:\n",
    "                topic_categories_freq[month]['None']+=1\n",
    "            except:\n",
    "                topic_categories_freq[month]['None']=1  \n",
    "      for k,v in categories.items():\n",
    "        if k not in temp:\n",
    "            topic_categories_freq[month][k]=0.0\n",
    "            topic_categories_score[month][k]=0.0\n",
    "    # Normalizing scores\n",
    "      for C in topic_categories_freq[month].keys():\n",
    "        if C is not 'None':\n",
    "            N = topic_categories_freq[month][C]\n",
    "            if N !=0:\n",
    "                topic_categories_score[month][C] = topic_categories_score[month][C]/N\n",
    "            else:\n",
    "                topic_categories_score[month][C] = 0.0\n",
    "      if topic_categories_freq[month]['agriculture'] == topic_categories_freq[month]['finance'] == topic_categories_freq[month]['mining'] == 0.0:\n",
    "        topic_categories_freq[month]['None']=1 \n",
    "      if topic_categories_score[month]['agriculture'] == topic_categories_score[month]['finance'] == topic_categories_score[month]['mining'] == 0.0:\n",
    "        topic_categories_score[month]['None']=1 \n",
    "#print(topic_categories_freq)\n",
    "#print(topic_categories_score)\n",
    "print ('Done assigning scores...')\n",
    "    # doc_cats[idx][cat] ; \n",
    "    #seeds[str(topic)], seeds_score[str(topic)] = calculate_seed(dtm_tf,doc_topic,tf_terms,topic,categories,seed_ratio) # return Dictionary [categories]:{docs} ==> nested dict\n",
    "    #clusters[str(topic)], centroids[str(topic)], mse[str(topic)] = ss_clustering(vsm_topics,seeds[str(topic)],doc_topic,topic) # k from len(seeds[str(topic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates a pie chart and saves it as a png file to be loaded later\n",
    "import numpy as np\n",
    "fileNameTemplate = path+'\\Plot{0:2s}'\n",
    "categoryy_list=categories.keys()\n",
    "categoryy_list=list(categoryy_list)\n",
    "for month in date_list:\n",
    "     print(\"For month\",month)\n",
    "     print('Visualization based on Category frequency: ', flush = True)\n",
    "     y=np.array(list(topic_categories_freq[month].values()))\n",
    "     percent = 100.*y/y.sum()\n",
    "     indices=np.where(percent == 0)[0].tolist()\n",
    "     porcent=np.delete(percent,indices)\n",
    "     np.asarray(categoryy_list)\n",
    "     category_list=np.delete(categoryy_list,indices).tolist()\n",
    "     labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(category_list, porcent)]\n",
    "     print(list(topic_categories_freq[month].keys()))\n",
    "     #labels = list(topic_categories_freq_c[country][str(topic)].keys())\n",
    "     #print(labels)\n",
    "     sizes = list(topic_categories_freq[month].values())\n",
    "     colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue','lightskyblue'][:len(labels)]\n",
    "     explode = (0.1, 0, 0, 0)  # explode 1st slice: Change this anyway you like\n",
    "     patches,text=plt.pie(sizes, colors=colors, startangle=140)\n",
    "     plt.axis('equal')\n",
    "     sort_legend = True\n",
    "     if sort_legend:\n",
    "        patches, labels, dummy =  zip(*sorted(zip(patches, labels, sizes),\n",
    "                                          key=lambda x: x[2],\n",
    "                                          reverse=True))\n",
    "\n",
    "     plt.legend(patches, labels, loc='left center', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=8)\n",
    "\n",
    "     plt.savefig(fileNameTemplate.format(str(month)+'.png'), format='png', bbox_inches='tight')\n",
    "     plt.show()\n",
    "print('Done Visualising and saving pie charts...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loads back the saved png pie charts to be displayed over month-topic groups axes\n",
    "import glob\n",
    "import numpy as np\n",
    "import ntpath\n",
    "import matplotlib.image as mpimg\n",
    "import os.path\n",
    "n_clusters=5      #the one specified in kmm\n",
    "fig, axarr = plt.subplots(nrows=1,ncols= len(date_list), sharex='col', sharey='row',  figsize=(40,5))\n",
    "c=0   \n",
    "for col in date_list:\n",
    "      axarr[c].xaxis.set_label_position(\"top\")\n",
    "      axarr[c].set_ylabel(col)\n",
    "      img=fileNameTemplate.format(str(col))+'.png'\n",
    "      o=axarr[c].imshow(mpimg.imread(img),interpolation='nearest',aspect='auto')\n",
    "      plt.setp(axarr[c].get_xticklabels(), visible=False)\n",
    "      plt.setp(axarr[c].get_yticklabels(), visible=False)\n",
    "      c+=1\n",
    "plt.show()\n",
    "print(\"Done plotting for months...\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
