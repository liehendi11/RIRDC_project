{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:819: DeprecationWarning: invalid escape sequence \\w\n",
      "  if re.match('\\w:', url) or re.match(r'\\\\', url):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:947: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', token) or re.search('\\s$', token):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:947: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', token) or re.search('\\s$', token):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:3650: DeprecationWarning: invalid escape sequence \\|\n",
      "  elif re.match('(or|\\|\\|)', conditional):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:3880: DeprecationWarning: invalid escape sequence \\.\n",
      "  name = re.sub('\\..*$', '', name)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:5229: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', string) or re.search('\\s$', string):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:5229: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', string) or re.search('\\s$', string):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\sharedstrings.py:103: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', string) or re.search('\\s$', string):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\sharedstrings.py:103: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', string) or re.search('\\s$', string):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\comments.py:160: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', text) or re.search('\\s$', text):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\comments.py:160: DeprecationWarning: invalid escape sequence \\s\n",
      "  if re.search('^\\s', text) or re.search('\\s$', text):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py:180: DeprecationWarning: invalid escape sequence \\[\n",
      "  _match = re.compile(\"(datetime64|M8)\\[(?P<unit>.+), (?P<tz>.+)\\]\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py:292: DeprecationWarning: invalid escape sequence \\[\n",
      "  _match = re.compile(\"(P|p)eriod\\[(?P<freq>.+)\\]\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py:410: DeprecationWarning: invalid escape sequence \\[\n",
      "  _match = re.compile(\"(I|i)nterval\\[(?P<subtype>.+)\\]\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numexpr\\cpuinfo.py:109: DeprecationWarning: invalid escape sequence \\d\n",
      "  nbits = re.compile('(\\d+)bit').search(abits).group(1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numexpr\\cpuinfo.py:662: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\\s+stepping\\s+(?P<STP>\\d+)\", re.IGNORECASE)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:399: DeprecationWarning: invalid escape sequence \\w\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:678: DeprecationWarning: invalid escape sequence \\d\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:767: DeprecationWarning: invalid escape sequence \\d\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2841: DeprecationWarning: invalid escape sequence \\*\n",
      "  \"\"\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:969: DeprecationWarning: invalid escape sequence \\m\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1007: DeprecationWarning: invalid escape sequence \\m\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1030: DeprecationWarning: invalid escape sequence \\c\n",
      "  buf.write('\\cline{{{0:d}-{1:d}}}\\n'.format(cl[1], icol))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\window.py:840: DeprecationWarning: invalid escape sequence \\*\n",
      "  \\*args and \\*\\*kwargs are passed to the function\"\"\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py:2696: DeprecationWarning: invalid escape sequence \\*\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:845: DeprecationWarning: invalid escape sequence \\d\n",
      "  def wide_to_long(df, stubnames, i, j, sep=\"\", suffix='\\d+'):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:154: DeprecationWarning: invalid escape sequence \\s\n",
      "  NaN: '\"\"\" + fill(\"', '\".join(sorted(_NA_VALUES)),\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:839: DeprecationWarning: invalid escape sequence \\s\n",
      "  result['delimiter'] = '\\s+'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:834: DeprecationWarning: invalid escape sequence \\s\n",
      "  \" different from '\\s+' are\"\\\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:827: DeprecationWarning: invalid escape sequence \\s\n",
      "  if engine == 'c' and sep == '\\s+':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\clipboard\\clipboard.py:6: DeprecationWarning: invalid escape sequence \\s\n",
      "  def read_clipboard(sep='\\s+', **kwargs):  # pragma: no cover\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\clipboard\\clipboard.py:61: DeprecationWarning: invalid escape sequence \\s\n",
      "  sep = '\\s+'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:1766: DeprecationWarning: invalid escape sequence \\d\n",
      "  m = re.search(\"values_block_(\\d+)\", name)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:4263: DeprecationWarning: invalid escape sequence \\d\n",
      "  _re_levels = re.compile(\"^level_\\d+$\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1305: DeprecationWarning: invalid escape sequence \\s\n",
      "  pat = re.compile('\\s+')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\testing.py:1874: DeprecationWarning: invalid escape sequence \\d\n",
      "  numeric_tuple = re.sub(\"[^\\d_]_?\", \"\", x).split(\"_\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\testing.py:2445: DeprecationWarning: invalid escape sequence \\(\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\t_sne.py:475: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initializes python packages and give paths to appropriate files to be used like categories file,\n",
    "the twitter file to be used alongwith the default stopword list (This is where our own created stoplist will be declared too).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import datetime, calendar, nltk, string, math, csv, glob, ntpath, os.path\n",
    "import numpy as np\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer; wnl = WordNetLemmatizer()\n",
    "import warnings, matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from rirdc_lib_old import cleanUp, calculate_seed\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\n9553291\\Documents'\n",
    "stopwords_file, categories_file = path+'\\\\stopwords.txt', path+'\\\\Category_DomainTerms.txt' # {(category, term)}\n",
    "out_fname,out_fname_cluster = path+'\\\\new_data_file.csv',path+'\\\\cluster_data_file.csv'\n",
    "extra_cats_file=path+'\\\\extra_cats.txt'\n",
    "\n",
    "#Give the wordnet path here\n",
    "wordnet_path=path+'\\\\corpora\\\\wn-domains-3.2\\\\wn-domains-3.2-20070223'\n",
    "\n",
    "n_topics, seed_ratio = 5, 0.25\n",
    "max_df, min_df = 0.5, 0.1 # For the VSM\n",
    "topic_groups=[]\n",
    "\n",
    "def keywithmaxval(d):\n",
    "    v=list(d.values());k=list(d.keys())\n",
    "    return k[v.index(max(v))]\n",
    "\n",
    "value=int(input(\"1 for twitter and 2 for patents\"))\n",
    "if value==1:\n",
    "    data_file=path+'\\\\results-20170725-145833.csv'\n",
    "elif value==2:\n",
    "    data_file=path+'\\\\urlpatentout2017_1.csv'\n",
    "print('Loading packages and file paths declarations done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a new file from the tweet file by getting rid of rows that are incomplete by appending their content to the \n",
    "#preceding row and deleting blank rows\n",
    "test_file=open(data_file,encoding=\"ISO-8859-1\")\n",
    "twitter_file = open(out_fname_cluster, 'w',encoding=\"ISO-8859-1\",newline='')\n",
    "reader = csv.reader(test_file, delimiter=',')\n",
    "writer = csv.writer(twitter_file)\n",
    "next(reader,None)\n",
    "prev=reader\n",
    "for row in reader:\n",
    "    if row[0] in (None, \"\"):\n",
    "        continue\n",
    "    for term in row[3]:\n",
    "        if term.isalpha()==False or ord(term) > 127:\n",
    "            row[3]=row[3].replace(term,' ')\n",
    "        wnl.lemmatize(term,'n')\n",
    "    writer.writerow(row)\n",
    "test_file.close()\n",
    "twitter_file.close()\n",
    "print('Done Creating the cleaned version of tweet file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Wordnet domains.\n",
    "s2d = []\n",
    "for i in open(wordnet_path, 'r'):\n",
    "    ssid, doms = i.strip().split('\\t')\n",
    "    doms = doms.split()\n",
    "    for d in doms:\n",
    "        s2d.append(d)\n",
    "synset2domains=list(set(s2d))\n",
    "extra_cats={}\n",
    "\n",
    "for syn in synset2domains:\n",
    "    try:\n",
    "        p_list,c_list,categories=[],[],[]\n",
    "        obj=wn.synset(syn+'.n.01')\n",
    "        topics=obj.topic_domains()\n",
    "        parents=obj.hypernyms()\n",
    "        children=obj.hyponyms()\n",
    "        domains= [ topic.lemma_names() for topic in topics ] \n",
    "        for sublist in domains:\n",
    "            for item in sublist:\n",
    "                    categories.append(str(item.replace('_',' ')))\n",
    "        pwords = [ parent.lemma_names() for parent in parents ]\n",
    "        for sublist in pwords:\n",
    "            for item in sublist:\n",
    "                    p_list.append(str(item.replace('_',' ')))\n",
    "        cwords = [ child.lemma_names() for child in children]\n",
    "        for sublist in cwords:\n",
    "            for item in sublist:\n",
    "                    c_list.append(str(item.replace('_',' ')))\n",
    "        words=p_list+c_list\n",
    "        if categories:\n",
    "            for category in categories:\n",
    "                extra_cats[category]=words\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "out_extra_cats = open(extra_cats_file, 'w')\n",
    "for k in extra_cats.keys():\n",
    "    for v in extra_cats[k]:\n",
    "        out_extra_cats.write(k+\", \")\n",
    "        out_extra_cats.write(v+\"\\n\")\n",
    "out_extra_cats.close()\n",
    "print(\"Done creating extra categories..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the equal number of files for the unique months and locations present \n",
    "#in the data file and dumps corresponding tweets to the appropriate ones.\n",
    "\n",
    "input_file = open(out_fname_cluster,encoding=\"ISO-8859-1\")\n",
    "date_list,country_list=[],[]\n",
    "reader = csv.reader(input_file, delimiter=',')\n",
    "\n",
    "#Creates a unique list of months and countries from tweets\n",
    "for row in reader:\n",
    "    created_at,*rest=row[1].split(' ')           #splits the 'Created at' column to retrieve 'month'\n",
    "    year,months,date=created_at.split('-')\n",
    "    month=calendar.month_name[int(months)]\n",
    "    if month not in date_list:\n",
    "        date_list.append(str(month))\n",
    "'''\n",
    "    if row[6] not in country_list:\n",
    "        country_list.append(str(row[6]))\n",
    "'''\n",
    "#Creates csv files with names after months\n",
    "for month in date_list:\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'w', newline='',encoding=\"ISO-8859-1\")\n",
    "    writer = csv.writer(outfile, delimiter = ',')\n",
    "    writer.writerow([\"tweet_id\",\"month\",\"text\",\"country\"])\n",
    "    outfile.close()\n",
    "'''\n",
    "#Creates csv files with names after countries\n",
    "for country in country_list:\n",
    "   outfile = open(path+'\\Data{0:2s}'.format(str(country))+'.csv', 'w', newline='',encoding=\"ISO-8859-1\")\n",
    "   writer = csv.writer(outfile, delimiter = ',')\n",
    "   writer.writerow([\"tweet_id\",\"month\",\"text\",\"country\"])\n",
    "   outfile.close()\n",
    "'''\n",
    "#Shifts rows of tweets to their respective csv files  -->  For Months\n",
    "for month1 in date_list:\n",
    "  with open(out_fname_cluster, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month1))+'.csv','a', newline='',encoding=\"ISO-8859-1\")\n",
    "    for row in reader:\n",
    "        if row[0] in (None, \"\"):\n",
    "          continue\n",
    "        created_at,*rest=row[1].split(' ')  \n",
    "        year,months,date=created_at.split('-')\n",
    "        month2=calendar.month_name[int(months)]\n",
    "        if month1==month2:\n",
    "            writer = csv.writer(outfile, delimiter =',')\n",
    "            writer.writerow(row)\n",
    "    outfile.close()\n",
    "'''\n",
    "#Shifts rows of tweets to their respective csv files  -->  For Countries\n",
    "for country1 in country_list:\n",
    "  with open(out_fname, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(country1))+'.csv','a', newline='',encoding=\"ISO-8859-1\")\n",
    "    for row in reader:\n",
    "        if row[0] in (None, \"\"):\n",
    "          continue\n",
    "        country2=str(row[6])\n",
    "        if country1==country2:\n",
    "            writer = csv.writer(outfile, delimiter =',')\n",
    "            writer.writerow(row)\n",
    "    outfile.close()\n",
    "'''\n",
    "print('Done moving tweets to their respective months and location files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizes and remove stopwords from the tweet column of the data file for bigrams and unigrams --> For Months\n",
    "\n",
    "df=open(stopwords_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "stops=df.readlines(); df.close()\n",
    "stops = set([word.strip() for word in stops])                  #stores stopwords tokens from the stoplist in a list\n",
    "stops1=list(stops)\n",
    "stops2=''.join(stops1)\n",
    "tweet_id_m, text_m={},{}\n",
    "tweet_id_c,text_c={},{}\n",
    "docs,r=[],[]\n",
    "sentences=[]\n",
    "DocZ_m, DocZ_c, DocZ_m_wob, DocZ_c_wob = {},{},{},{} # will be used to find categories\n",
    "for month in date_list:\n",
    "    u,p,d,d_wob=[],[],[],[]\n",
    "    outfile = open(path+'\\Data{0:2s}'.format(str(month))+'.csv', 'r', newline='',encoding=\"ISO-8859-1\")\n",
    "    reader = csv.reader(outfile, delimiter=',')\n",
    "    next(reader,None)\n",
    "    docs=list(reader);\n",
    "    for doc in docs:    \n",
    "        tmp = [t.strip() for t in doc]\n",
    "        try:\n",
    "            if len(tmp[3])>0: # Making sure not blanks\n",
    "                sentence = [word\n",
    "                    for word in nltk.word_tokenize(tmp[3].lower())\n",
    "                    if word not in string.punctuation\n",
    "                           and len(word)>3\n",
    "                           ]\n",
    "                filtered_word_list = sentence[:] #make a copy of the word_list\n",
    "                for term in sentence: # iterate over word_list\n",
    "                    if term in stops2: \n",
    "                        filtered_word_list.remove(term)\n",
    "                d_wob.append(tmp[3].lower())                  #Unigrams List\n",
    "                sentences.append(filtered_word_list)                  #Bigrams List\n",
    "                filtered=' '.join(filtered_word_list)\n",
    "                u.append(tmp[0]); p.append(tmp[3]);d.append(filtered)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_id_m[month]=u\n",
    "    text_m[month]=p \n",
    "    DocZ_m[month]=d\n",
    "    DocZ_m_wob[month]=d_wob\n",
    "   \n",
    "print(\"For the months found\")\n",
    "print(\"The bigrams are -->\", DocZ_m)\n",
    "#print(\"The unigrams are -->\", DocZ_m_wob)\n",
    "\n",
    "#Gets the top five bigrams\n",
    "flat_list = [item for sublist in sentences for item in sublist]\n",
    "word_counter = {}\n",
    "for word in flat_list:\n",
    "     if word in word_counter:\n",
    "        word_counter[word] += 1\n",
    "     else:\n",
    "         word_counter[word] = 1\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "top_5 = popular_words[:5]\n",
    "print(\"The top five bigrams are\",top_5)\n",
    "del u,p,d,d_wob,sentence\n",
    "    \n",
    "del docs\n",
    "print(\"Clean-up for months done...,\", end = ' ', flush = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the corpus into vector space using tf-idf\n",
    "\n",
    "tfidf_vectorizer,tfidf_matrix,terms={},{},{}\n",
    "for m in date_list:\n",
    "    tfidf_vectorizer[m] = TfidfVectorizer(max_df=0.5, max_features=200000,\n",
    "                                 min_df=10,stop_words='english', ngram_range=(1,3))\n",
    "    tfidf_matrix[m] = tfidf_vectorizer[m].fit_transform(DocZ_m[m]) #fit the vectorizer to tweets\n",
    "    terms[m] = tfidf_vectorizer[m].get_feature_names()\n",
    "print(terms)\n",
    "print(\"VSM done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering the documents using the k-means algorithm\n",
    "\n",
    "clusters={}\n",
    "num_clusters = 5        #specify the number of clusters here\n",
    "km = KMeans(n_clusters=num_clusters,random_state=4000)\n",
    "for m in date_list:\n",
    "    km.fit(tfidf_matrix[m])\n",
    "    clusters[m] = km.labels_.tolist()\n",
    "print(\"Clustering done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dictionary for cluster-docs pair\n",
    "\n",
    "tweets,frame,mydict={},{},{}\n",
    "for m in date_list:\n",
    "    tweets[m] = { 'tweets': DocZ_m[m], 'cluster': clusters[m] }\n",
    "    frame[m] = pd.DataFrame(tweets[m], index=None,columns = ['tweets', 'cluster'])\n",
    "    #frame['cluster'].value_counts()\n",
    "    mydict[m]={k: list(v) for k,v in frame[m].groupby(\"cluster\")[\"tweets\"]}\n",
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading categories into category dictionary of set of terms for bigrams and unigrams \n",
    "\n",
    "print(\"Please wait (it's going to take a while): Loading Data,\", end = ' ', flush = True)\n",
    "df=open(categories_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); categories = {}\n",
    "\n",
    "\n",
    "for cat in cats:\n",
    "     key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "     try:        \n",
    "          categories[key].add(term)\n",
    "     except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file\n",
    "del cats,df\n",
    "'''\n",
    "df=open(extra_cats_file,\"r\",encoding=\"ISO-8859-1\", errors='replace')\n",
    "cats=df.readlines(); df.close(); \n",
    "for cat in cats:\n",
    "     key = cat.split(', ')[0].strip(); term = cat.split(', ')[1].strip().lower()\n",
    "     try:        \n",
    "          categories[key].add(term)\n",
    "     except:\n",
    "          categories[key] = set([term])               #stores categories-terms as a key-value in a dictionary from the file\n",
    "del cats,df\n",
    "\n",
    "print(\"pre-Categories mapping ... \", flush = True)    \n",
    "doc_cats_m,doc_cat_m = {}, {}; \n",
    "doc_cats_m_wob,doc_cat_m_wob={},{};\n",
    "for month in date_list:\n",
    "  doc_cats_m[month],doc_cat_m[month]={},{} \n",
    "  for cluster_id in range(0,num_clusters):\n",
    "     doc_cats_m[month][cluster_id],doc_cat_m[month][cluster_id]={},{}\n",
    "     for idx,doc in enumerate(mydict[month][cluster_id]):\n",
    "        notInCategories,total=[],0\n",
    "        doc_cats_m[month][cluster_id][idx]={}\n",
    "        for cat, terms in categories.items():\n",
    "            for term in terms:\n",
    "                n = doc.count(term)              #calculates the frequency of occurance of terms corresponding to terms for the category\n",
    "                total+=n\n",
    "                try:\n",
    "                    doc_cats_m[month][cluster_id][idx][cat] += n\n",
    "                except:\n",
    "                    doc_cats_m[month][cluster_id][idx][cat]= n\n",
    "        for cat, terms in categories.items():\n",
    "            if total==0:\n",
    "                doc_cat_m[month][cluster_id][idx] = None\n",
    "                doc_cats_m[month][cluster_id][idx][cat] = 0.0\n",
    "                if month not in notInCategories:\n",
    "                    notInCategories.append(month)\n",
    "            else:\n",
    "                doc_cat_m[month][cluster_id][idx] = keywithmaxval(doc_cats_m[month][cluster_id][idx])           # category of doc[idx]\n",
    "                doc_cats_m[month][cluster_id][idx][cat] = doc_cats_m[month][cluster_id][idx][cat]/total          # Normalizing to category's ratio\n",
    "                \n",
    "del doc\n",
    "print(doc_cat_m)\n",
    "print('%d/%d Documents were not in any of the categories.' %(len(notInCategories), len(mydict)), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates a score for each domain for each document\n",
    "topic_categories_freq, topic_categories_score = {}, {}\n",
    "for month in date_list:\n",
    " topic_categories_freq[month],topic_categories_score[month]={},{}\n",
    " for cluster_id in range(0,num_clusters):\n",
    "#topics_groups = [[1,12],[2,18],[3,7,9,14,15,19],[4],[5],[6,8,11],[10,13,16,17],[20]] # Change this depending on the visualization result; PS: this is \"LDAtopic+1\"\n",
    "#topics_scores, seeds, seeds_score, clusters, centroids, mse = {}, {}, {}, {}, {}, {}\n",
    "      topic_categories_freq[month][cluster_id], topic_categories_score[month][cluster_id] = {}, {}\n",
    "      temp=[]  \n",
    "      for k,v in doc_cat_m[month][cluster_id].items():\n",
    "        C=doc_cat_m[month][cluster_id][k]\n",
    "        temp.append(doc_cat_m[month][cluster_id][k])\n",
    "        if C: #not None\n",
    "            try:\n",
    "                topic_categories_freq[month][cluster_id][C]+=1\n",
    "                topic_categories_score[month][cluster_id][C]+=doc_cats_m[month][cluster_id][k][C]\n",
    "            except:\n",
    "                topic_categories_freq[month][cluster_id][C]=1\n",
    "                topic_categories_score[month][cluster_id][C]=doc_cats_m[month][cluster_id][k][C]\n",
    "        else:\n",
    "            try:\n",
    "                topic_categories_freq[month][cluster_id]['None']+=1\n",
    "            except:\n",
    "                topic_categories_freq[month][cluster_id]['None']=1  \n",
    "      for k,v in categories.items():\n",
    "        if k not in temp:\n",
    "            topic_categories_freq[month][cluster_id][k]=0.0\n",
    "            topic_categories_score[month][cluster_id][k]=0.0\n",
    "    # Normalizing scores\n",
    "      for C in topic_categories_freq[month][cluster_id].keys():\n",
    "        if C is not 'None':\n",
    "            N = topic_categories_freq[month][cluster_id][C]\n",
    "            if N !=0:\n",
    "                topic_categories_score[month][cluster_id][C] = topic_categories_score[month][cluster_id][C]/N\n",
    "            else:\n",
    "                topic_categories_score[month][cluster_id][C] = 0.0\n",
    "      if topic_categories_freq[month][cluster_id]['agriculture'] == topic_categories_freq[month][cluster_id]['finance'] == topic_categories_freq[month][cluster_id]['mining'] == 0.0:\n",
    "        topic_categories_freq[month][cluster_id]['None']=1 \n",
    "      if topic_categories_score[month][cluster_id]['agriculture'] == topic_categories_score[month][cluster_id]['finance'] == topic_categories_score[month][cluster_id]['mining'] == 0.0:\n",
    "        topic_categories_score[month][cluster_id]['None']=1 \n",
    "print(topic_categories_freq)\n",
    "print(topic_categories_score)\n",
    "print ('Done assigning scores...')\n",
    "    # doc_cats[idx][cat] ; \n",
    "    #seeds[str(topic)], seeds_score[str(topic)] = calculate_seed(dtm_tf,doc_topic,tf_terms,topic,categories,seed_ratio) # return Dictionary [categories]:{docs} ==> nested dict\n",
    "    #clusters[str(topic)], centroids[str(topic)], mse[str(topic)] = ss_clustering(vsm_topics,seeds[str(topic)],doc_topic,topic) # k from len(seeds[str(topic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "words_dict,words,termss={},{},{}\n",
    "for month in date_list:\n",
    "    tfidf_vectorizer[month] = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "                                 min_df=9,stop_words='english', ngram_range=(1,3))\n",
    "    termss[month] = {}\n",
    "    words_dict[month],words[month]={},{}\n",
    "    for cluster_id in range(0,num_clusters):\n",
    "        tfidf_matrix[month] = tfidf_vectorizer[month].fit_transform(mydict[month][cluster_id])\n",
    "        km.fit(tfidf_matrix[month])\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "        vocab_frame = pd.DataFrame({'words': mydict[month][cluster_id]}, index = mydict[month][cluster_id])\n",
    "        #print(\"Before\",vocab_frame)\n",
    "        vocab_frame.dropna(axis=0, how='any')\n",
    "        #print(\"After\",vocab_frame)\n",
    "        words_dict[month][cluster_id]={}\n",
    "        words[month][cluster_id]={}\n",
    "        termss[month][cluster_id]=tfidf_vectorizer[month].get_feature_names()\n",
    "        #sort cluster centers by proximity to centroid\n",
    "        for ind in order_centroids[cluster_id, :6]: #replace 6 with n words per cluster\n",
    "            words[month][cluster_id]=str(vocab_frame.ix[termss[month][cluster_id][ind].split(' ')].values.tolist()[0][0]).encode('ascii', 'ignore').decode()\n",
    "        if cluster_id not in words_dict[month]:\n",
    "            words_dict[month][cluster_id]=[]\n",
    "        #print(words)\n",
    "    for idx in range(0,num_clusters):\n",
    "        word=words[month][idx]\n",
    "        if word in words_dict[month][idx] or word=='nan':\n",
    "                continue\n",
    "        else:\n",
    "                words_dict[month][idx]=word\n",
    "print(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a pie chart and saves it as a png file to be loaded later\n",
    "\n",
    "fileNameTemplate = path+'\\Plot{0:s}'\n",
    "categoryy_list=categories.keys()\n",
    "categoryy_list=list(categoryy_list)\n",
    "for month in date_list:\n",
    "    print(\"For month\",month)\n",
    "    for cluster_id in range(0,num_clusters):\n",
    "        print(\"For Cluster ID\",str(cluster_id))\n",
    "        print('Visualization based on Category frequency: ', flush = True)\n",
    "        y=np.array(list(topic_categories_freq[month][cluster_id].values()))\n",
    "        percent = 100.*y/y.sum()\n",
    "        indices=np.where(percent == 0)[0].tolist()\n",
    "        porcent=np.delete(percent,indices)\n",
    "        np.asarray(categoryy_list)\n",
    "        category_list=np.delete(categoryy_list,indices).tolist()\n",
    "        labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(category_list, porcent)]\n",
    "        sizes = list(topic_categories_freq[month][cluster_id].values())\n",
    "        cs=cm.Set1(np.arange(len(labels)))\n",
    "        explode = (0.1, 0, 0, 0)  # explode 1st slice: Change this anyway you like\n",
    "        patches,text=plt.pie(sizes, colors=cs, startangle=140)\n",
    "        plt.title(words_dict[month][cluster_id])\n",
    "        plt.axis('equal')\n",
    "        sort_legend = True\n",
    "        if sort_legend:\n",
    "            patches, labels, dummy =  zip(*sorted(zip(patches, labels, sizes),\n",
    "                                              key=lambda x: x[2],\n",
    "                                              reverse=True))\n",
    "\n",
    "        plt.legend(patches, labels, loc='left center', bbox_to_anchor=(-0.1, 1.),\n",
    "               fontsize=8)\n",
    "        plt.savefig(fileNameTemplate.format(str(month)+'-'+str(cluster_id)+'.png'), format='png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "print('Done Visualising and saving pie charts...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loads back the saved png pie charts to be displayed over cluster axis\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=len(date_list),ncols= num_clusters, sharex='col', sharey='row',  figsize=(40,5))\n",
    "c=0   \n",
    "r=0\n",
    "if len(date_list)==1:\n",
    "    axarr[0].yaxis.set_label_position(\"left\")\n",
    "    axarr[0].set_ylabel(date_list[0])\n",
    "    c=0\n",
    "    for col in range(0,num_clusters):\n",
    "        ttl=''\n",
    "        for top in col:\n",
    "            ttl=ttl+mydict[str(top)]+','\n",
    "        axarr[c].set_title(ttl)\n",
    "        img=fileNameTemplate.format(str(col))+'-'+date_list[0]+'.png'\n",
    "        o=axarr[c].imshow(mpimg.imread(img),interpolation='nearest', aspect='auto')\n",
    "        plt.setp(axarr[c].get_xticklabels(), visible=False)\n",
    "        plt.setp(axarr[c].get_yticklabels(), visible=False)\n",
    "        c+=1\n",
    "    \n",
    "#else use numpy multi-dimesnional array\n",
    "else:\n",
    "     r=0\n",
    "     for row in date_list:\n",
    "        c=0\n",
    "        axarr[r,0].yaxis.set_label_position(\"left\")\n",
    "        axarr[r,0].set_ylabel(row)\n",
    "        for col in range(0,num_clusters):  \n",
    "            ttl=''\n",
    "            ttl=ttl+str(words_dict[row][col])+','\n",
    "            axarr[r,c].set_title(ttl)\n",
    "            img=fileNameTemplate.format(str(row)+'-'+str(col))+'.png'\n",
    "            o=axarr[r,c].imshow(mpimg.imread(img),interpolation='nearest',aspect='auto')\n",
    "            plt.setp(axarr[r,c].get_xticklabels(), visible=False)\n",
    "            plt.setp(axarr[r,c].get_yticklabels(), visible=False)\n",
    "            c+=1\n",
    "        r+=1\n",
    "plt.show()\n",
    "print(\"Done plotting for months...\")\n",
    "print(\"Done plotting ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
